{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulation-saying",
   "metadata": {},
   "source": [
    "## Gaussion Process Regression\n",
    "\n",
    "This notebook is used for performing various GP experiments and saving the output for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "senior-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "\n",
    "from synapse_utils import io\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.infer import TraceMeanField_ELBO\n",
    "from pyro.infer.util import torch_backward, torch_item\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# from sklearn.cluster import KMeans\n",
    "from cuml import KMeans\n",
    "\n",
    "# assert pyro.__version__.startswith('1.7.0')\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-trust",
   "metadata": {},
   "source": [
    "## Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sublime-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = '../..'\n",
    "run_id = 'synapseclr__so3__seed_42__second_stage'\n",
    "checkpoint_path = f'../../output/checkpoint__{run_id}'\n",
    "output_root = f'../../output/checkpoint__{run_id}/analysis/gp'\n",
    "\n",
    "dataset_path = '../../data/MICrONS__L23__8_8_40__processed'\n",
    "contamination_indices_path = '../../tables/meta_df_contamination_indices.npy'\n",
    "\n",
    "reload_epoch = 99\n",
    "node_idx_list = [0, 1, 2, 3]\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "\n",
    "training_fraction = 0.9\n",
    "perform_class_balancing = True\n",
    "perform_pca = False\n",
    "perform_kmeans = True\n",
    "n_pca_features = 50\n",
    "k_fold = 3\n",
    "random_seed = 42\n",
    "\n",
    "kernel_type = 'rbf'\n",
    "z_jitter = 0.1\n",
    "elbo_type = 'mean-field'\n",
    "\n",
    "# initial kernel parameters\n",
    "init_gaussian_variance = 0.5\n",
    "init_rbf_variance = 1.0\n",
    "init_rbf_lengthscale = 0.5\n",
    "init_linear_variance = 1.0\n",
    "init_constant_variance = 1.0\n",
    "init_laplace_variance = 1.0\n",
    "init_laplace_lengthscale = 0.5\n",
    "\n",
    "print_loss_every = 1000\n",
    "eval_every = 1000\n",
    "\n",
    "lr = 0.001\n",
    "num_optim_steps = 10_000 + 1\n",
    "\n",
    "trait_key_list = [\n",
    "    'cleft_size_log1p_zscore',\n",
    "    'presyn_soma_dist_log1p_zscore',\n",
    "    'postsyn_soma_dist_log1p_zscore',\n",
    "    'mito_size_pre_vx_log1p_zscore_zi',\n",
    "    'mito_size_post_vx_log1p_zscore_zi',\n",
    "    'pre_and_post_cell_types',\n",
    "    'pre_cell_type',\n",
    "    'post_cell_type',\n",
    "    'has_mito_pre',\n",
    "    'has_mito_post'\n",
    "]\n",
    "\n",
    "trait_type_list = [\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical'\n",
    "]\n",
    "\n",
    "trait_num_categories_list = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "trait_control_list = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    'has_mito_pre',\n",
    "    'has_mito_post',\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None\n",
    "]\n",
    "\n",
    "def get_augmented_table(meta_ext_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # combined pre and post cell types\n",
    "    pre_post_cell_types_map = {\n",
    "        (0, 0): 0,\n",
    "        (0, 1): 1,\n",
    "        (1, 0): 2,\n",
    "        (1, 1): 3,\n",
    "    }\n",
    "    \n",
    "    pre_cell_type_values = meta_ext_df['pre_cell_type'].values\n",
    "    post_cell_type_values = meta_ext_df['post_cell_type'].values\n",
    "    pre_post_cell_type_values = np.asarray(list(\n",
    "        map(pre_post_cell_types_map.get,\n",
    "            zip(pre_cell_type_values, post_cell_type_values))))\n",
    "    \n",
    "    aug_meta_ext_df = meta_ext_df.copy()\n",
    "    aug_meta_ext_df['pre_and_post_cell_types'] = pre_post_cell_type_values\n",
    "    \n",
    "    return aug_meta_ext_df\n",
    "\n",
    "def generate_manifest(var_dict: dict) -> dict:    \n",
    "    attributes = [\n",
    "        'experiment_prefix',\n",
    "        'experiment_desc',\n",
    "        'experiment_output_root',\n",
    "        'checkpoint_path',\n",
    "        'reload_epoch',\n",
    "        'feature_hook',\n",
    "        'l2_normalize',\n",
    "        'k_fold',\n",
    "        'perform_class_balancing',\n",
    "        'perform_pca',\n",
    "        'n_pca_features',\n",
    "        'z_jitter',\n",
    "        'init_rbf_variance',\n",
    "        'init_rbf_lengthscale',\n",
    "        'init_gaussian_variance',\n",
    "        'init_linear_variance',\n",
    "        'init_constant_variance',\n",
    "        'kernel_type',\n",
    "        'elbo_type',\n",
    "        'lr',\n",
    "        'num_optim_steps',\n",
    "        'trait_key_list',\n",
    "        'trait_type_list',\n",
    "        'trait_num_categories_list',\n",
    "        'trait_control_list',\n",
    "        'n_inducing_points_list',\n",
    "        'random_seed',\n",
    "        'num_optim_steps'\n",
    "    ]\n",
    "    \n",
    "    manifest = {attribute: var_dict[attribute] for attribute in attributes}\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ae0f67-5ed3-42f4-971e-231785cfe712",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_manifest_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exceptional-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first wave\n",
    "experiment_prefix = 'first_wave'\n",
    "experiment_desc = 'synapse_simclr_rbf_different_features'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_pca = False\n",
    "\n",
    "kernel_type = 'rbf'\n",
    "n_inducing_points = 1000\n",
    "n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "\n",
    "for feature_hook, l2_normalize in [\n",
    "        ('encoder.fc', False),\n",
    "        ('projector.mlp.0', False),\n",
    "        ('projector.mlp.3', False),\n",
    "        ('projector.mlp.3', True)]:\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)\n",
    "    \n",
    "n_inducing_points_ = 50\n",
    "n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "\n",
    "for feature_hook, l2_normalize in [\n",
    "        ('encoder.fc', False),\n",
    "        ('projector.mlp.0', False),\n",
    "        ('projector.mlp.3', False),\n",
    "        ('projector.mlp.3', True)]:\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cdbb8f-f3af-4444-96e5-81e6545ee3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collected-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second wave\n",
    "experiment_prefix = 'second_wave'\n",
    "experiment_desc = 'synapse_simclr_different_kernels'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_pca = False\n",
    "\n",
    "kernel_type = 'laplace'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "for n_inducing_points in [10, 20, 50, 100, 200, 500]:\n",
    "    n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)\n",
    "\n",
    "kernel_type = 'linear'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "for n_inducing_points in [10, 5]:\n",
    "    n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a60f84-f114-4db5-826e-97bc83916238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "concerned-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third wave\n",
    "experiment_prefix = 'third_wave'\n",
    "experiment_desc = 'synapse_simclr_rbf_pca'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_pca = True\n",
    "kernel_type = 'rbf'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "for n_pca_features in [50, 100]:\n",
    "    for n_inducing_points in [50, 500]:\n",
    "        n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "        manifest = generate_manifest(locals())\n",
    "        experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90a2f68-0a09-4793-b552-c3cb387f104b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hawaiian-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth wave\n",
    "experiment_prefix = 'fourth_wave'\n",
    "experiment_desc = 'synapse_simclr_rbf_n_inducing'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_pca = False\n",
    "kernel_type = 'rbf'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "for n_inducing_points in [10, 20, 50, 100, 200, 5, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11231026-ad71-43b7-a9e4-ef51b1b3afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "superb-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth wave\n",
    "experiment_prefix = 'fifth_wave'\n",
    "experiment_desc = 'medicalnet_rbf_n_inducing'\n",
    "checkpoint_path = f'../../output/checkpoint__medicalnet'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "node_idx_list = [0]\n",
    "reload_epoch = 0\n",
    "perform_pca = False\n",
    "kernel_type = 'rbf'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "for n_inducing_points in [5, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de423bfa-825c-4da4-9ea1-fcba04a36cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intermediate-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sixth wave\n",
    "experiment_prefix = 'sixth_wave'\n",
    "experiment_desc = 'random_rbf_n_inducing'\n",
    "checkpoint_path = f'../../output/checkpoint__random'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "node_idx_list = [0]\n",
    "reload_epoch = 0\n",
    "perform_pca = False\n",
    "kernel_type = 'rbf'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "for n_inducing_points in [5, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    n_inducing_points_list = [n_inducing_points] * len(trait_key_list)\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a7a12aa-10ed-4b8a-b002-7296d7442e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c059002-e41e-47df-b7ab-1414f861444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth wave\n",
    "experiment_prefix = 'ninth_wave'\n",
    "experiment_desc = 'synapse_simclr_rbf_preset_n_inducing'\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_pca = False\n",
    "kernel_type = 'rbf'\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "n_inducing_points_list = [\n",
    "    400,\n",
    "    100,\n",
    "    200,\n",
    "    300,\n",
    "    10,\n",
    "    100,\n",
    "    100,\n",
    "    100,\n",
    "    200,\n",
    "    300\n",
    "]\n",
    "\n",
    "manifest = generate_manifest(locals())\n",
    "experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c379ee-29d5-46a1-9e8c-6b82f635650b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "extended-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "save_extended = False\n",
    "notebook_mode = True\n",
    "\n",
    "if not notebook_mode:\n",
    "    start_experiment_index = int(sys.argv[1])\n",
    "    end_experiment_index = int(sys.argv[2])\n",
    "\n",
    "else:\n",
    "    start_experiment_index = 0\n",
    "    end_experiment_index = len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_container(container, key, entry):\n",
    "    if key not in container:\n",
    "        container[key] = []\n",
    "    container[key].append(entry)\n",
    "\n",
    "for experiment_index, manifest in list(\n",
    "        enumerate(experiment_manifest_list))[start_experiment_index:end_experiment_index]:\n",
    "\n",
    "    # set local variables from the manifest\n",
    "    for key, value in manifest.items():\n",
    "        setattr(sys.modules[__name__], key, value)\n",
    "\n",
    "    # basic checks\n",
    "    n_traits = len(trait_key_list)\n",
    "    assert len(trait_type_list) == n_traits\n",
    "    assert len(trait_control_list) == n_traits\n",
    "\n",
    "    # announce\n",
    "    print(f'Starting experiment {experiment_index} ...')\n",
    "    print(manifest)\n",
    "    print()\n",
    "\n",
    "    # load features\n",
    "    features_nf, meta_df, meta_ext_df = io.load_features(\n",
    "        checkpoint_path,\n",
    "        node_idx_list,\n",
    "        reload_epoch,\n",
    "        feature_hook=feature_hook,\n",
    "        dataset_path=dataset_path,\n",
    "        l2_normalize=l2_normalize,\n",
    "        contamination_indices_path=contamination_indices_path)\n",
    "        \n",
    "    if perform_pca:\n",
    "        print('Performing PCA ...')\n",
    "        features_nf = PCA(n_pca_features).fit_transform(features_nf)\n",
    "    \n",
    "    # add combined columns to the table (if necessary)\n",
    "    meta_ext_df = get_augmented_table(meta_ext_df)\n",
    "    \n",
    "    # generating cross-validation data splits\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    n_traits = len(trait_key_list)\n",
    "\n",
    "    synapse_ids_to_meta_ext_df_row_idx_map = {\n",
    "        synapse_id: row_idx\n",
    "        for row_idx, synapse_id in enumerate(meta_ext_df['synapse_id'].values)}\n",
    "\n",
    "    synapse_ids_to_meta_df_row_idx_map = {\n",
    "        synapse_id: row_idx\n",
    "        for row_idx, synapse_id in enumerate(meta_df['synapse_id'].values)}\n",
    "\n",
    "    train_meta_ext_df_dict = dict()\n",
    "    test_meta_ext_df_dict = dict()\n",
    "\n",
    "    for i in range(n_traits):\n",
    "\n",
    "        trait_key = trait_key_list[i]\n",
    "        trait_type = trait_type_list[i]\n",
    "        trait_num_categories = trait_num_categories_list[i]\n",
    "        trait_control = trait_control_list[i]\n",
    "\n",
    "        if trait_type == 'categorical':\n",
    "            per_category_indices = [\n",
    "                np.nonzero(meta_ext_df[trait_key].values == category_index)[0]\n",
    "                for category_index in range(trait_num_categories)]\n",
    "        else:\n",
    "            per_category_indices = None\n",
    "\n",
    "        for k in range(k_fold):\n",
    "\n",
    "            # if continuous, no class balancing is needed\n",
    "            if trait_type == 'continuous':\n",
    "\n",
    "                n_annotated = len(meta_ext_df)\n",
    "                n_train = int(n_annotated * training_fraction)\n",
    "                n_test = n_annotated - n_train\n",
    "                perm = rng.permutation(n_annotated)\n",
    "                train_indices = perm[:n_train]\n",
    "                test_indices = perm[n_train:]\n",
    "\n",
    "            # if categorical, perform class balancing\n",
    "            elif trait_type == 'categorical':\n",
    "\n",
    "                if perform_class_balancing:\n",
    "\n",
    "                    n_annotated = len(meta_ext_df)\n",
    "                    n_train = int(n_annotated * training_fraction)\n",
    "                    n_test = n_annotated - n_train\n",
    "                    n_train_per_category = n_train // trait_num_categories\n",
    "                    n_test_per_category = n_test // trait_num_categories\n",
    "\n",
    "                    train_indices = []\n",
    "                    test_indices = []\n",
    "\n",
    "                    for category_index in range(trait_num_categories):\n",
    "\n",
    "                        # partition the category conditional annotations into disjoint test and train groups\n",
    "                        n_annotated = len(per_category_indices[category_index])\n",
    "                        n_train = int(n_annotated * training_fraction)\n",
    "                        n_test = n_annotated - n_train\n",
    "                        assert n_train > 0\n",
    "                        assert n_test > 0\n",
    "\n",
    "                        perm = rng.permutation(n_annotated)\n",
    "                        all_train_indices = per_category_indices[category_index][perm[:n_train]]\n",
    "                        all_test_indices = per_category_indices[category_index][perm[n_train:]]\n",
    "\n",
    "                        train_indices += rng.choice(\n",
    "                            all_train_indices, replace=True, size=n_train_per_category).tolist()\n",
    "                        test_indices += rng.choice(\n",
    "                            all_test_indices, replace=True, size=n_test_per_category).tolist()\n",
    "\n",
    "                else:\n",
    "\n",
    "                    n_annotated = len(meta_ext_df)\n",
    "                    n_train = int(n_annotated * training_fraction)\n",
    "                    n_test = n_annotated - n_train\n",
    "                    perm = rng.permutation(n_annotated)\n",
    "                    train_indices = perm[:n_train]\n",
    "                    test_indices = perm[n_train:]\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            rng.shuffle(train_indices)\n",
    "            rng.shuffle(test_indices)\n",
    "\n",
    "            train_meta_ext_df_dict[(i, k)] = meta_ext_df.iloc[train_indices].copy().reset_index(drop=True)\n",
    "            test_meta_ext_df_dict[(i, k)] = meta_ext_df.iloc[test_indices].copy().reset_index(drop=True)\n",
    "\n",
    "    # container for evaluations\n",
    "    eval_container_dict = dict()\n",
    "    loss_container_dict = dict()\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        for trait_index in range(n_traits):\n",
    "\n",
    "            # setup\n",
    "            trait_key = trait_key_list[trait_index]\n",
    "            trait_type = trait_type_list[trait_index]\n",
    "            trait_num_categories = trait_num_categories_list[trait_index]\n",
    "            trait_control = trait_control_list[trait_index]\n",
    "\n",
    "            train_meta_ext_df = train_meta_ext_df_dict[(trait_index, k)]\n",
    "            test_meta_ext_df = test_meta_ext_df_dict[(trait_index, k)]\n",
    "\n",
    "            assert trait_type in {'continuous', 'categorical'}\n",
    "\n",
    "            print(f'Running GP for {trait_key}, type = {trait_type}, fold = {k}, control = {trait_control}')\n",
    "\n",
    "            # do we need to censor the train and test data?\n",
    "            if trait_control is not None:\n",
    "                train_meta_ext_df = train_meta_ext_df[train_meta_ext_df[trait_control] == 1]\n",
    "                test_meta_ext_df = test_meta_ext_df[test_meta_ext_df[trait_control] == 1]\n",
    "                assert len(train_meta_ext_df) > 0\n",
    "                assert len(test_meta_ext_df) > 0\n",
    "\n",
    "            train_trait_values_n = torch.tensor(\n",
    "                train_meta_ext_df[trait_key].values,\n",
    "                device=device, dtype=dtype)\n",
    "\n",
    "            test_trait_values_n = torch.tensor(\n",
    "                test_meta_ext_df[trait_key].values,\n",
    "                device=device, dtype=dtype)\n",
    "\n",
    "            print(f'Number of training data points: {len(train_trait_values_n)}')\n",
    "            print(f'Number of test data points: {len(test_trait_values_n)}')\n",
    "\n",
    "            # select the corresponding representations\n",
    "            train_indices = list(map(synapse_ids_to_meta_df_row_idx_map.get, train_meta_ext_df['synapse_id'].values))\n",
    "            test_indices = list(map(synapse_ids_to_meta_df_row_idx_map.get, test_meta_ext_df['synapse_id'].values))\n",
    "            train_z_nf = torch.tensor(\n",
    "                features_nf[train_indices],\n",
    "                device=device, dtype=dtype)\n",
    "            test_z_nf = torch.tensor(\n",
    "                features_nf[test_indices],\n",
    "                device=device, dtype=dtype)\n",
    "\n",
    "            ### run GP ##\n",
    "\n",
    "            # initialize the inducing inputs\n",
    "            x_dim = features_nf.shape[-1]\n",
    "\n",
    "            # k-means selection of inducing points\n",
    "            n_inducing_points = n_inducing_points_list[trait_index]\n",
    "            print(f'Number of inducing points for {trait_key_list[trait_index]}: {n_inducing_points}')\n",
    "\n",
    "            if perform_kmeans:\n",
    "                print('Performing k-means ...')\n",
    "                Xu_init_kf = KMeans(n_clusters=n_inducing_points).fit(features_nf).cluster_centers_\n",
    "                print('Done!')\n",
    "\n",
    "            else:\n",
    "                print('Selecting random inducing points ...')\n",
    "                Xu_init_kf = torch.tensor(\n",
    "                    features_nf[rng.permutation(len(features_nf))[:n_inducing_points]],\n",
    "                    device=device, dtype=dtype)\n",
    "                Xu_init_kf = Xu_init_kf + z_jitter * torch.randn_like(Xu_init_kf)\n",
    "                Xu_init_kf = Xu_init_kf.detach().cpu().numpy()\n",
    "\n",
    "            # select a subset of synapse representations + random jitter as inducing points\n",
    "            Xu = torch.tensor(Xu_init_kf, device=device, dtype=dtype)\n",
    "\n",
    "            # set the covariates (X) to the representations\n",
    "            X = train_z_nf\n",
    "\n",
    "            # set the readout (y) to the trait\n",
    "            y = train_trait_values_n\n",
    "\n",
    "            # initialize the kernel, likelihood, and model\n",
    "            pyro.clear_param_store()\n",
    "\n",
    "            if trait_type == 'continuous':\n",
    "                likelihood = gp.likelihoods.Gaussian(\n",
    "                    variance=torch.tensor(init_gaussian_variance))\n",
    "                latent_shape = None\n",
    "\n",
    "            elif trait_type == 'categorical':\n",
    "                likelihood = gp.likelihoods.MultiClass(num_classes=trait_num_categories)\n",
    "                latent_shape = (trait_num_categories,)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            # instantiate the GP model\n",
    "            if kernel_type == 'rbf':\n",
    "                rbf_kernel = gp.kernels.RBF(\n",
    "                    input_dim=x_dim,\n",
    "                    variance=torch.tensor(init_rbf_variance),\n",
    "                    lengthscale=torch.tensor(init_rbf_lengthscale))\n",
    "                kernel = rbf_kernel\n",
    "\n",
    "            elif kernel_type == 'linear':\n",
    "                linear_kernel = gp.kernels.Linear(\n",
    "                    input_dim=x_dim,\n",
    "                    variance=torch.tensor(init_linear_variance))\n",
    "                constant_kernel = gp.kernels.Constant(\n",
    "                    input_dim=x_dim,\n",
    "                    variance=torch.tensor(init_constant_variance))\n",
    "                kernel = gp.kernels.Sum(linear_kernel, constant_kernel)\n",
    "\n",
    "            elif kernel_type == 'laplace':\n",
    "                laplace_kernel = gp.kernels.Exponential(\n",
    "                    input_dim=x_dim,\n",
    "                    variance=torch.tensor(init_laplace_variance),\n",
    "                    lengthscale=torch.tensor(init_laplace_lengthscale))\n",
    "                kernel = laplace_kernel\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            kernel = kernel.to(device)\n",
    "            vsgp = gp.models.VariationalSparseGP(\n",
    "                X, y, kernel,\n",
    "                Xu=Xu,\n",
    "                likelihood=likelihood,\n",
    "                whiten=True,\n",
    "                jitter=1e-4,\n",
    "                latent_shape=latent_shape).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(vsgp.parameters(), lr=lr)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_optim_steps)\n",
    "\n",
    "            if elbo_type == 'mean-field':\n",
    "                loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "\n",
    "            elif elbo_type == 'map':\n",
    "                loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(vsgp.model, vsgp.guide)\n",
    "                torch_backward(loss)\n",
    "                return loss\n",
    "\n",
    "            loss_container_dict[(trait_index, k)] = []\n",
    "            eval_container_dict[(trait_index, k)] = dict()\n",
    "\n",
    "            for i_iter in range(num_optim_steps):\n",
    "\n",
    "                # otpimizer step\n",
    "                loss = optimizer.step(closure)\n",
    "\n",
    "                # log\n",
    "                if i_iter % print_loss_every == 0:\n",
    "                    print(f'iter: {i_iter}, lr: {scheduler.get_last_lr()[0]:.5f}, loss: {torch_item(loss)}')\n",
    "\n",
    "                # save loss\n",
    "                loss_container_dict[(trait_index, k)].append((i_iter, float(torch_item(loss))))\n",
    "\n",
    "                # scheduler step\n",
    "                scheduler.step()\n",
    "\n",
    "                # evaluate\n",
    "                if i_iter % eval_every == 0:\n",
    "\n",
    "                    for eval_set in {'train', 'test'}:\n",
    "\n",
    "                        if eval_set == 'test':\n",
    "                            X_test = test_z_nf\n",
    "                            y_test = test_trait_values_n\n",
    "\n",
    "                        elif eval_set == 'train':\n",
    "                            X_test = train_z_nf\n",
    "                            y_test = train_trait_values_n\n",
    "\n",
    "                        else:\n",
    "                            raise ValueError\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            y_test_pred_mean, y_test_pred_cov = vsgp(X_test, full_cov=False)\n",
    "                            y_test_pred_sd = y_test_pred_cov.sqrt()\n",
    "\n",
    "                        if trait_type == 'continuous':\n",
    "                            residual_variance = torch.var(y_test_pred_mean - y_test).item()\n",
    "                            total_variance = torch.var(y_test).item()\n",
    "                            explained_variance = 1. - residual_variance / total_variance\n",
    "                            append_to_container(\n",
    "                                eval_container_dict[(trait_index, k)], f'{eval_set}_explained_variance', (i_iter, float(explained_variance)))\n",
    "                            print(f'\\t[{eval_set} eval] explained variance: {explained_variance:3f}')\n",
    "\n",
    "                        elif trait_type == 'categorical':\n",
    "                            y_test_pred_soft = torch.softmax(y_test_pred_mean, dim=0).cpu().numpy()\n",
    "                            y_test_pred_hard = torch.softmax(y_test_pred_mean, dim=0).argmax(dim=0).cpu().numpy()\n",
    "                            y_test_hard = y_test.type(torch.int).cpu().numpy()\n",
    "\n",
    "                            # calculate confusion matrix\n",
    "                            if save_extended:\n",
    "                                confusion_matrix = np.zeros((trait_num_categories, trait_num_categories))\n",
    "                                for actual_category, pred_category in zip(y_test_hard, y_test_pred_hard):\n",
    "                                    confusion_matrix[actual_category, pred_category] += 1\n",
    "                                append_to_container(\n",
    "                                    eval_container_dict[(trait_index, k)], f'{eval_set}_confusion_matrix', (i_iter, confusion_matrix))\n",
    "\n",
    "                            # calculate ROC curve and AUCROC\n",
    "                            for i_category in range(trait_num_categories):\n",
    "                                scores = y_test_pred_soft[i_category, :]\n",
    "                                actual = (y_test_hard == i_category).astype(int)\n",
    "                                fpr, tpr, threshold = roc_curve(actual, scores)\n",
    "                                auc = roc_auc_score(actual, scores)\n",
    "                                append_to_container(\n",
    "                                    eval_container_dict[(trait_index, k)], f'{eval_set}_{i_category}_roc_auc', (i_iter, float(auc)))\n",
    "                                if save_extended:\n",
    "                                    append_to_container(\n",
    "                                        eval_container_dict[(trait_index, k)], f'{eval_set}_{i_category}_roc_fpr', (i_iter, fpr))\n",
    "                                    append_to_container(\n",
    "                                        eval_container_dict[(trait_index, k)], f'{eval_set}_{i_category}_roc_tpr', (i_iter, tpr))\n",
    "                                    append_to_container(\n",
    "                                        eval_container_dict[(trait_index, k)], f'{eval_set}_{i_category}_roc_thresholds', (i_iter, threshold))\n",
    "\n",
    "                                print(f'\\t[{eval_set} eval] category {i_category} AUCROC: {auc:3f}')\n",
    "\n",
    "                        else:\n",
    "                            raise ValueError\n",
    "\n",
    "    # save the results\n",
    "    if save_results:\n",
    "        \n",
    "        output_file_name = f'experiment__{experiment_prefix}__{experiment_index}.pkl'\n",
    "\n",
    "        with open(os.path.join(experiment_output_root, output_file_name), 'wb') as f:\n",
    "            pickle.dump(manifest, f)\n",
    "            pickle.dump(eval_container_dict, f)\n",
    "            pickle.dump(loss_container_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf61c4-73e5-4890-8592-c8fd92cc10f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-rapids-22.08-py",
   "name": "common-cu113.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-22.08]",
   "language": "python",
   "name": "conda-env-rapids-22.08-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
