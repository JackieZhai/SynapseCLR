{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integral-silence",
   "metadata": {},
   "source": [
    "## Gaussion Process Regression\n",
    "\n",
    "Trains the optimal GP model as determined by the previous experiments for:\n",
    "- several training data resamples with different censoring rates (for cross-validation)\n",
    "- several training data resamples with no censoring (\"production run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emotional-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "\n",
    "from synapse_utils import io\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.infer import TraceMeanField_ELBO\n",
    "from pyro.infer.util import torch_backward, torch_item\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from cuml import KMeans\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# assert pyro.__version__.startswith('1.7.0')\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-pointer",
   "metadata": {},
   "source": [
    "## Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = '../..'\n",
    "run_id = 'synapseclr__so3__seed_42__second_stage'\n",
    "checkpoint_path = f'../../output/checkpoint__{run_id}'\n",
    "output_root = f'../../output/checkpoint__{run_id}/analysis/gp'\n",
    "\n",
    "dataset_path = '../../data/MICrONS__L23__8_8_40__processed'\n",
    "contamination_indices_path = '../../tables/meta_df_contamination_indices.npy'\n",
    "\n",
    "reload_epoch = 99\n",
    "node_idx_list = [0, 1, 2, 3]\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "\n",
    "perform_class_balancing = True\n",
    "perform_pca = False\n",
    "n_pca_features = 50\n",
    "k_fold = 1\n",
    "random_seed = 42\n",
    "kernel_type = 'rbf'\n",
    "z_jitter = 0.05\n",
    "elbo_type = 'mean-field'\n",
    "\n",
    "# initial kernel parameters\n",
    "init_gaussian_variance = 0.1\n",
    "init_rbf_variance = 1.0\n",
    "init_rbf_lengthscale = 0.5\n",
    "init_linear_variance = 1.0\n",
    "init_constant_variance = 1.0\n",
    "init_laplace_variance = 1.0\n",
    "init_laplace_lengthscale = 0.5\n",
    "\n",
    "print_loss_every = 1000\n",
    "eval_every = 1000\n",
    "\n",
    "lr = 0.001\n",
    "num_optim_steps = 20_000 + 1\n",
    "\n",
    "\n",
    "def get_augmented_table(meta_ext_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # combined pre and post cell types\n",
    "    pre_post_cell_types_map = {\n",
    "        (0, 0): 0,\n",
    "        (0, 1): 1,\n",
    "        (1, 0): 2,\n",
    "        (1, 1): 3,\n",
    "    }\n",
    "    \n",
    "    pre_cell_type_values = meta_ext_df['pre_cell_type'].values\n",
    "    post_cell_type_values = meta_ext_df['post_cell_type'].values\n",
    "    pre_post_cell_type_values = np.asarray(list(\n",
    "        map(pre_post_cell_types_map.get,\n",
    "            zip(pre_cell_type_values, post_cell_type_values))))\n",
    "    \n",
    "    aug_meta_ext_df = meta_ext_df.copy()\n",
    "    aug_meta_ext_df['pre_and_post_cell_types'] = pre_post_cell_type_values\n",
    "    \n",
    "    return aug_meta_ext_df\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def get_censored_table(\n",
    "        meta_ext_df: pd.DataFrame,\n",
    "        rng: np.random.RandomState,\n",
    "        censored_fraction: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    n_entries = len(meta_ext_df)\n",
    "    n_censored_entries = int(np.ceil(n_entries * censored_fraction))\n",
    "    perm = rng.permutation(n_entries)\n",
    "    censored_indices = perm[:n_censored_entries]\n",
    "    kept_indices = perm[n_censored_entries:]\n",
    "    \n",
    "    return (\n",
    "        meta_ext_df.iloc[kept_indices].copy().reset_index(drop=True),\n",
    "        meta_ext_df.iloc[censored_indices].copy().reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def generate_manifest(var_dict: dict) -> dict:\n",
    "    attributes = [\n",
    "        'experiment_prefix',\n",
    "        'experiment_desc',\n",
    "        'experiment_output_root',\n",
    "        'checkpoint_path',\n",
    "        'reload_epoch',\n",
    "        'feature_hook',\n",
    "        'l2_normalize',\n",
    "        'k_fold',\n",
    "        'perform_class_balancing',\n",
    "        'perform_pca',\n",
    "        'n_pca_features',\n",
    "        'z_jitter',\n",
    "        'init_rbf_variance',\n",
    "        'init_rbf_lengthscale',\n",
    "        'init_gaussian_variance',\n",
    "        'init_linear_variance',\n",
    "        'init_constant_variance',\n",
    "        'kernel_type',\n",
    "        'elbo_type',\n",
    "        'lr',\n",
    "        'num_optim_steps',\n",
    "        'trait_key_list',\n",
    "        'trait_type_list',\n",
    "        'trait_num_categories_list',\n",
    "        'trait_control_list',\n",
    "        'n_inducing_points_list',\n",
    "        'censored_fraction',\n",
    "        'random_seed'\n",
    "    ]\n",
    "    manifest = {attribute: var_dict[attribute] for attribute in attributes}\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186e352d-f695-48a9-bd80-f69de4b6d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the list\n",
    "experiment_manifest_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cheap-nelson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n"
     ]
    }
   ],
   "source": [
    "experiment_prefix = 'seventh_wave'\n",
    "experiment_desc = 'synapse_simclr_consensus'\n",
    "\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_class_balancing = True\n",
    "perform_pca = False\n",
    "perform_kmeans = True\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "z_jitter = 0.05\n",
    "training_fraction = 1.0\n",
    "n_inducing_points = 50\n",
    "random_seed = 42\n",
    "kernel_type = 'rbf'\n",
    "elbo_type = 'mean-field'\n",
    "num_optim_steps = 20_000 + 1\n",
    "impute_split_size = 10000\n",
    "training_fraction = 1.0\n",
    "censored_fraction = 0.1\n",
    "\n",
    "\n",
    "trait_key_list = [\n",
    "    'pre_cell_type',\n",
    "    'post_cell_type',\n",
    "]\n",
    "\n",
    "trait_type_list = [\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "]\n",
    "\n",
    "trait_num_categories_list = [\n",
    "    2,\n",
    "    2,\n",
    "]\n",
    "\n",
    "trait_control_list = [\n",
    "    None,\n",
    "    None,\n",
    "]\n",
    "\n",
    "n_inducing_points_list = [\n",
    "    100,\n",
    "    100,\n",
    "]\n",
    "\n",
    "n_total = 94874\n",
    "n_annotations = 5623\n",
    "labeled_fraction_list = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "for labeled_fraction in labeled_fraction_list:\n",
    "    for random_seed in [46, 47, 48, 49, 50]:\n",
    "        censored_fraction = (n_annotations - labeled_fraction * n_total) / n_annotations\n",
    "        print(f'Fraction of censored data: {censored_fraction:.3f}')\n",
    "        manifest = generate_manifest(locals())\n",
    "        experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3022bb9c-efbc-4066-abb8-9c4064b024c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southeast-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_prefix = 'eighth_wave'\n",
    "experiment_desc = 'synapse_simclr_production'\n",
    "\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_class_balancing = True\n",
    "perform_pca = False\n",
    "perform_kmeans = True\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "z_jitter = 0.05\n",
    "training_fraction = 1.0\n",
    "n_inducing_points = 50\n",
    "random_seed = 42\n",
    "kernel_type = 'rbf'\n",
    "elbo_type = 'mean-field'\n",
    "num_optim_steps = 20_000 + 1\n",
    "impute_split_size = 10000\n",
    "\n",
    "training_fraction = 1.0\n",
    "censored_fraction = 0.0\n",
    "\n",
    "trait_key_list = [\n",
    "    'cleft_size_log1p_zscore',\n",
    "    'presyn_soma_dist_log1p_zscore',\n",
    "    'postsyn_soma_dist_log1p_zscore',\n",
    "    'mito_size_pre_vx_log1p_zscore_zi',\n",
    "    'mito_size_post_vx_log1p_zscore_zi',\n",
    "    'pre_and_post_cell_types',\n",
    "    'pre_cell_type',\n",
    "    'post_cell_type',\n",
    "    'has_mito_pre',\n",
    "    'has_mito_post'\n",
    "]\n",
    "\n",
    "trait_type_list = [\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'continuous',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "    'categorical'\n",
    "]\n",
    "\n",
    "trait_num_categories_list = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "]\n",
    "\n",
    "trait_control_list = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    'has_mito_pre',\n",
    "    'has_mito_post',\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None\n",
    "]\n",
    "\n",
    "n_inducing_points_list = [\n",
    "    400,\n",
    "    100,\n",
    "    200,\n",
    "    300,\n",
    "    10,\n",
    "    100,\n",
    "    100,\n",
    "    100,\n",
    "    200,\n",
    "    300\n",
    "]\n",
    "\n",
    "n_total = 94874\n",
    "n_annotations = 5623\n",
    "\n",
    "for random_seed in [40, 41, 42, 43, 45]:\n",
    "    manifest = generate_manifest(locals())\n",
    "    experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae08b561-bcbb-43cf-81eb-acb45642ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04933b81-4b53-486b-b8b9-d554d772282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.831\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.663\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.494\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.325\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n",
      "Fraction of censored data: 0.156\n"
     ]
    }
   ],
   "source": [
    "experiment_prefix = 'tenth_wave'\n",
    "experiment_desc = 'synapse_simclr_consensus'\n",
    "\n",
    "experiment_output_root = os.path.join(output_root, experiment_desc)\n",
    "os.makedirs(experiment_output_root, exist_ok=True)\n",
    "\n",
    "perform_class_balancing = True\n",
    "perform_pca = False\n",
    "perform_kmeans = True\n",
    "feature_hook = 'encoder.fc'\n",
    "l2_normalize = False\n",
    "z_jitter = 0.05\n",
    "training_fraction = 1.0\n",
    "n_inducing_points = 50\n",
    "random_seed = 42\n",
    "kernel_type = 'rbf'\n",
    "elbo_type = 'mean-field'\n",
    "num_optim_steps = 20_000 + 1\n",
    "impute_split_size = 10000\n",
    "training_fraction = 1.0\n",
    "censored_fraction = 0.1\n",
    "\n",
    "\n",
    "trait_key_list = [\n",
    "    'pre_cell_type',\n",
    "    'post_cell_type',\n",
    "]\n",
    "\n",
    "trait_type_list = [\n",
    "    'categorical',\n",
    "    'categorical',\n",
    "]\n",
    "\n",
    "trait_num_categories_list = [\n",
    "    2,\n",
    "    2,\n",
    "]\n",
    "\n",
    "trait_control_list = [\n",
    "    None,\n",
    "    None,\n",
    "]\n",
    "\n",
    "n_inducing_points_list = [\n",
    "    100,\n",
    "    100,\n",
    "]\n",
    "\n",
    "n_total = 94874\n",
    "n_annotations = 5623\n",
    "labeled_fraction_list = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "for labeled_fraction in labeled_fraction_list:\n",
    "    for random_seed in [40, 41, 42, 43, 45]:\n",
    "        censored_fraction = (n_annotations - labeled_fraction * n_total) / n_annotations\n",
    "        print(f'Fraction of censored data: {censored_fraction:.3f}')\n",
    "        manifest = generate_manifest(locals())\n",
    "        experiment_manifest_list.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6a7feb-1551-44e7-8903-77bcd389fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_manifest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_index in range(len(experiment_manifest_list)):\n",
    "    \n",
    "    manifest = experiment_manifest_list[experiment_index]\n",
    "\n",
    "    # set local variables from the manifest\n",
    "    for key, value in manifest.items():\n",
    "        setattr(sys.modules[__name__], key, value)\n",
    "        \n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    # basic checks\n",
    "    n_traits = len(trait_key_list)\n",
    "    assert len(trait_type_list) == n_traits\n",
    "    assert len(trait_control_list) == n_traits\n",
    "\n",
    "    # announce\n",
    "    print(f'Starting experiment {experiment_index} ...')\n",
    "    print(manifest)\n",
    "    print()\n",
    "\n",
    "    # load features\n",
    "    features_nf, meta_df, meta_ext_df = io.load_features(\n",
    "        checkpoint_path,\n",
    "        node_idx_list,\n",
    "        reload_epoch,\n",
    "        feature_hook=feature_hook,\n",
    "        dataset_path=dataset_path,\n",
    "        l2_normalize=l2_normalize,\n",
    "        contamination_indices_path=contamination_indices_path)\n",
    "\n",
    "    # add combined columns to the table (if necessary)\n",
    "    meta_ext_df = get_augmented_table(meta_ext_df)\n",
    "\n",
    "    # censor / keep\n",
    "    meta_ext_df, censored_meta_ext_df = get_censored_table(\n",
    "        meta_ext_df, rng, censored_fraction)\n",
    "\n",
    "    synapse_ids_to_meta_ext_df_row_idx_map = {\n",
    "        synapse_id: row_idx\n",
    "        for row_idx, synapse_id in enumerate(meta_ext_df['synapse_id'].values)}\n",
    "\n",
    "    synapse_ids_to_meta_df_row_idx_map = {\n",
    "        synapse_id: row_idx\n",
    "        for row_idx, synapse_id in enumerate(meta_df['synapse_id'].values)}\n",
    "\n",
    "    # pre-processing\n",
    "    if perform_pca:\n",
    "        features_nf = PCA(n_pca_features).fit_transform(features_nf)\n",
    "\n",
    "    train_meta_ext_df_dict = dict()\n",
    "    test_meta_ext_df_dict = dict()\n",
    "\n",
    "    for i in range(n_traits):\n",
    "\n",
    "        trait_key = trait_key_list[i]\n",
    "        trait_type = trait_type_list[i]\n",
    "        trait_num_categories = trait_num_categories_list[i]\n",
    "        trait_control = trait_control_list[i]\n",
    "\n",
    "        # make a train dataframe\n",
    "        train_meta_ext_df = meta_ext_df.copy()\n",
    "\n",
    "        # censor by trait control\n",
    "        if trait_control is not None:\n",
    "            train_meta_ext_df = train_meta_ext_df[train_meta_ext_df[trait_control] == 1]\n",
    "\n",
    "        if trait_type == 'categorical':\n",
    "            per_category_indices = [\n",
    "                np.nonzero(train_meta_ext_df[trait_key].values == category_index)[0]\n",
    "                for category_index in range(trait_num_categories)]\n",
    "        else:\n",
    "            per_category_indices = None\n",
    "\n",
    "        # if continuous, no class balancing is needed\n",
    "        if trait_type == 'continuous':\n",
    "\n",
    "            n_annotated = len(train_meta_ext_df)\n",
    "            n_train = int(n_annotated * training_fraction)\n",
    "            n_test = n_annotated - n_train\n",
    "            perm = rng.permutation(n_annotated)\n",
    "            train_indices = perm[:n_train]\n",
    "            test_indices = perm[n_train:]\n",
    "\n",
    "        # if categorical, perform class balancing\n",
    "        elif trait_type == 'categorical':\n",
    "\n",
    "            if perform_class_balancing:\n",
    "\n",
    "                n_annotated = len(train_meta_ext_df)\n",
    "                n_train = int(n_annotated * training_fraction)\n",
    "                n_test = n_annotated - n_train\n",
    "                n_train_per_category = n_train // trait_num_categories\n",
    "                n_test_per_category = n_test // trait_num_categories\n",
    "\n",
    "                train_indices = []\n",
    "                test_indices = []\n",
    "\n",
    "                for category_index in range(trait_num_categories):\n",
    "\n",
    "                    # partition the category conditional annotations into disjoint test and train groups\n",
    "                    n_annotated = len(per_category_indices[category_index])\n",
    "                    n_train = int(n_annotated * training_fraction)\n",
    "                    n_test = n_annotated - n_train\n",
    "\n",
    "                    perm = rng.permutation(n_annotated)\n",
    "                    all_train_indices = per_category_indices[category_index][perm[:n_train]]\n",
    "                    all_test_indices = per_category_indices[category_index][perm[n_train:]]\n",
    "\n",
    "                    train_indices += rng.choice(\n",
    "                        all_train_indices,\n",
    "                        replace=True,\n",
    "                        size=n_train_per_category).tolist()\n",
    "\n",
    "                    test_indices += rng.choice(\n",
    "                        all_test_indices,\n",
    "                        replace=True,\n",
    "                        size=n_test_per_category).tolist()\n",
    "\n",
    "            else:\n",
    "\n",
    "                n_annotated = len(train_meta_ext_df)\n",
    "                n_train = int(n_annotated * training_fraction)\n",
    "                n_test = n_annotated - n_train\n",
    "                perm = rng.permutation(n_annotated)\n",
    "                train_indices = perm[:n_train]\n",
    "                test_indices = perm[n_train:]\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        rng.shuffle(train_indices)\n",
    "        rng.shuffle(test_indices)\n",
    "\n",
    "        train_meta_ext_df_dict[i] = train_meta_ext_df.iloc[train_indices].copy().reset_index(drop=True)\n",
    "        test_meta_ext_df_dict[i] = train_meta_ext_df.iloc[test_indices].copy().reset_index(drop=True)\n",
    "\n",
    "    y_pred_dict = dict()\n",
    "\n",
    "    for trait_index in range(n_traits):\n",
    "\n",
    "        # setup\n",
    "        trait_key = trait_key_list[trait_index]\n",
    "        trait_type = trait_type_list[trait_index]\n",
    "        trait_num_categories = trait_num_categories_list[trait_index]\n",
    "        trait_control = trait_control_list[trait_index]\n",
    "\n",
    "        train_meta_ext_df = train_meta_ext_df_dict[trait_index]\n",
    "        test_meta_ext_df = test_meta_ext_df_dict[trait_index]\n",
    "\n",
    "        assert trait_type in {'continuous', 'categorical'}\n",
    "\n",
    "        print(f'Running GP for {trait_key}, type = {trait_type}, control = {trait_control}')\n",
    "\n",
    "        train_trait_values_n = torch.tensor(\n",
    "            train_meta_ext_df[trait_key].values,\n",
    "            device=device, dtype=dtype)\n",
    "\n",
    "        test_trait_values_n = torch.tensor(\n",
    "            test_meta_ext_df[trait_key].values,\n",
    "            device=device, dtype=dtype)\n",
    "\n",
    "        print(f'Number of training data points: {len(train_trait_values_n)}')\n",
    "        print(f'Number of test data points: {len(test_trait_values_n)}')\n",
    "\n",
    "        # select the corresponding representations\n",
    "        train_indices = list(map(synapse_ids_to_meta_df_row_idx_map.get, train_meta_ext_df['synapse_id'].values))\n",
    "        test_indices = list(map(synapse_ids_to_meta_df_row_idx_map.get, test_meta_ext_df['synapse_id'].values))\n",
    "        train_z_nf = torch.tensor(\n",
    "            features_nf[train_indices],\n",
    "            device=device, dtype=dtype)\n",
    "        test_z_nf = torch.tensor(\n",
    "            features_nf[test_indices],\n",
    "            device=device, dtype=dtype)\n",
    "\n",
    "        ### run GP ##\n",
    "\n",
    "        # initialize the inducing inputs\n",
    "        x_dim = features_nf.shape[-1]\n",
    "\n",
    "        # k-means selection of inducing points\n",
    "        n_inducing_points = n_inducing_points_list[trait_index]\n",
    "        print(f'Number of inducing points for {trait_key_list[trait_index]}: {n_inducing_points}')\n",
    "\n",
    "        if perform_kmeans:\n",
    "            print('Performing k-means ...')\n",
    "            Xu_init_kf = KMeans(n_clusters=n_inducing_points).fit(features_nf).cluster_centers_\n",
    "            print('Done!')\n",
    "\n",
    "        else:\n",
    "            Xu_init_kf = torch.tensor(\n",
    "                features_nf[rng.permutation(len(features_nf))[:n_inducing_points]],\n",
    "                device=device, dtype=dtype)\n",
    "            Xu_init_kf = Xu_init_kf + z_jitter * torch.randn_like(Xu_init_kf)\n",
    "            Xu_init_kf = Xu_init_kf.detach().cpu().numpy()\n",
    "        \n",
    "        # select a subset of synapse representations + random jitter as inducing points\n",
    "        Xu = torch.tensor(Xu_init_kf, device=device, dtype=dtype)\n",
    "\n",
    "        # set the covariates (X) to the representations\n",
    "        X = train_z_nf\n",
    "\n",
    "        # set the readout (y) to the trait\n",
    "        y = train_trait_values_n\n",
    "\n",
    "        # initialize the kernel, likelihood, and model\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        if trait_type == 'continuous':\n",
    "            likelihood = gp.likelihoods.Gaussian(\n",
    "                variance=torch.tensor(init_gaussian_variance))\n",
    "            latent_shape = None\n",
    "\n",
    "        elif trait_type == 'categorical':\n",
    "            likelihood = gp.likelihoods.MultiClass(num_classes=trait_num_categories)\n",
    "            latent_shape = (trait_num_categories,)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # instantiate the GP model\n",
    "        if kernel_type == 'rbf':\n",
    "            rbf_kernel = gp.kernels.RBF(\n",
    "                input_dim=x_dim,\n",
    "                variance=torch.tensor(init_rbf_variance),\n",
    "                lengthscale=torch.tensor(init_rbf_lengthscale))\n",
    "            kernel = rbf_kernel\n",
    "\n",
    "        elif kernel_type == 'linear':\n",
    "            linear_kernel = gp.kernels.Linear(\n",
    "                input_dim=x_dim,\n",
    "                variance=torch.tensor(init_linear_variance))\n",
    "            constant_kernel = gp.kernels.Constant(\n",
    "                input_dim=x_dim,\n",
    "                variance=torch.tensor(init_constant_variance))\n",
    "            kernel = gp.kernels.Sum(linear_kernel, constant_kernel)\n",
    "\n",
    "        elif kernel_type == 'laplace':\n",
    "            laplace_kernel = gp.kernels.Exponential(\n",
    "                input_dim=x_dim,\n",
    "                variance=torch.tensor(init_laplace_variance),\n",
    "                lengthscale=torch.tensor(init_laplace_lengthscale))\n",
    "            kernel = laplace_kernel\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        kernel = kernel.to(device)\n",
    "        vsgp = gp.models.VariationalSparseGP(\n",
    "            X, y, kernel,\n",
    "            Xu=Xu,\n",
    "            likelihood=likelihood,\n",
    "            whiten=True,\n",
    "            jitter=1e-4,\n",
    "            latent_shape=latent_shape).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(vsgp.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_optim_steps)\n",
    "\n",
    "        if elbo_type == 'mean-field':\n",
    "            loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "\n",
    "        elif elbo_type == 'map':\n",
    "            loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(vsgp.model, vsgp.guide)\n",
    "            torch_backward(loss)\n",
    "            return loss\n",
    "\n",
    "        for i_iter in range(num_optim_steps):\n",
    "\n",
    "            # otpimizer step\n",
    "            loss = optimizer.step(closure)\n",
    "\n",
    "            # log\n",
    "            if i_iter % print_loss_every == 0:\n",
    "                print(f'iter: {i_iter}, lr: {scheduler.get_last_lr()[0]:.5f}, loss: {torch_item(loss)}')\n",
    "\n",
    "            # scheduler step\n",
    "            scheduler.step()\n",
    "\n",
    "            # evaluate\n",
    "            if i_iter % eval_every == 0:\n",
    "\n",
    "                for eval_set in {'train', 'test'}:\n",
    "\n",
    "                    if eval_set == 'test':\n",
    "                        X_test = test_z_nf\n",
    "                        y_test = test_trait_values_n\n",
    "\n",
    "                    elif eval_set == 'train':\n",
    "                        X_test = train_z_nf\n",
    "                        y_test = train_trait_values_n\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "\n",
    "                    if len(X_test) == 0:\n",
    "                        continue\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        y_test_pred_mean, y_test_pred_cov = vsgp(X_test, full_cov=False)\n",
    "                        y_test_pred_sd = y_test_pred_cov.sqrt()\n",
    "\n",
    "                    if trait_type == 'continuous':\n",
    "                        residual_variance = torch.var(y_test_pred_mean - y_test).item()\n",
    "                        total_variance = torch.var(y_test).item()\n",
    "                        explained_variance = 1. - residual_variance / total_variance\n",
    "                        print(f'\\t[{eval_set} eval] explained variance: {explained_variance:3f}')\n",
    "\n",
    "                    elif trait_type == 'categorical':\n",
    "                        y_test_pred_soft = torch.softmax(y_test_pred_mean, dim=0).cpu().numpy()\n",
    "                        y_test_pred_hard = torch.softmax(y_test_pred_mean, dim=0).argmax(dim=0).cpu().numpy()\n",
    "                        y_test_hard = y_test.type(torch.int).cpu().numpy()\n",
    "\n",
    "                        # calculate confusion matrix\n",
    "                        confusion_matrix = np.zeros((trait_num_categories, trait_num_categories))\n",
    "                        for actual_category, pred_category in zip(y_test_hard, y_test_pred_hard):\n",
    "                            confusion_matrix[actual_category, pred_category] += 1\n",
    "\n",
    "                        # calculate ROC curve and AUCROC\n",
    "                        for i_category in range(trait_num_categories):\n",
    "                            scores = y_test_pred_soft[i_category, :]\n",
    "                            actual = (y_test_hard == i_category).astype(int)\n",
    "                            fpr, tpr, threshold = roc_curve(actual, scores)\n",
    "                            auc = roc_auc_score(actual, scores)\n",
    "                            print(f'\\t[{eval_set} eval] category {i_category} AUCROC: {auc:3f}')\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "\n",
    "        # impute!\n",
    "        y_pred_mean_list = []\n",
    "        y_pred_std_list = []\n",
    "\n",
    "        print(\"Imputing ...\")\n",
    "        for split_features_nf in torch.split(torch.tensor(features_nf), impute_split_size):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred_mean, y_pred_cov = vsgp(\n",
    "                    split_features_nf.to(device).type(dtype),\n",
    "                    full_cov=False)\n",
    "                y_pred_std = y_pred_cov.sqrt()\n",
    "\n",
    "                if trait_type == 'continuous':\n",
    "                    y_pred_mean_list.append(y_pred_mean.cpu().numpy())\n",
    "                    y_pred_std_list.append(y_pred_std.cpu().numpy())\n",
    "\n",
    "                elif trait_type == 'categorical':\n",
    "                    y_pred_mean_list.append(y_pred_mean.cpu().numpy().T)\n",
    "                    y_pred_std_list.append(y_pred_std.cpu().numpy().T)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "        y_pred_mean = np.concatenate(y_pred_mean_list, axis=0)\n",
    "        y_pred_std = np.concatenate(y_pred_std_list, axis=0)\n",
    "\n",
    "        if trait_type == 'continuous':\n",
    "            y_pred_dict[f'imputed__{trait_key}__mean'] = y_pred_mean\n",
    "            y_pred_dict[f'imputed__{trait_key}__std'] = y_pred_std\n",
    "\n",
    "        elif trait_type == 'categorical':\n",
    "            y_pred_mean = torch.softmax(torch.tensor(y_pred_mean), dim=-1).cpu().numpy()\n",
    "            for i_category in range(trait_num_categories):\n",
    "                y_pred_dict[f'imputed__{trait_key}__class_{i_category}'] = y_pred_mean[:, i_category]\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    imputed_meta_df = meta_df.copy()\n",
    "\n",
    "    for k, v in y_pred_dict.items():\n",
    "        imputed_meta_df[k] = v\n",
    "\n",
    "    imputed_meta_df = imputed_meta_df.drop(\n",
    "        ['n_cutout_sections',\n",
    "         'filename',\n",
    "         'post_synaptic_volume',\n",
    "         'pre_synaptic_volume',\n",
    "         'synaptic_cleft_volume'], axis=1)\n",
    "\n",
    "    imputed_meta_df.to_csv(\n",
    "        os.path.join(experiment_output_root, f'imputed_meta__{kernel_type}__{n_inducing_points}__c={censored_fraction:.3f}__s={random_seed}.csv'))\n",
    "    meta_ext_df.to_csv(\n",
    "        os.path.join(experiment_output_root, f'training_meta_ext__{kernel_type}__{n_inducing_points}__c={censored_fraction:.3f}__s={random_seed}.csv'))\n",
    "    censored_meta_ext_df.to_csv(\n",
    "        os.path.join(experiment_output_root, f'censored_meta_ext__{kernel_type}__{n_inducing_points}__c={censored_fraction:.3f}__s={random_seed}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3c0e3-80e5-4fa0-972f-095d122042e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-rapids-22.08-py",
   "name": "common-cu113.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-22.08]",
   "language": "python",
   "name": "conda-env-rapids-22.08-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
