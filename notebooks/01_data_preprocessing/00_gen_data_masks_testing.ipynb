{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synapse EM image chunks and segmentation layers\n",
    "\n",
    "Extract synapse EM image chunks and segmentation layers from MICrONS public dataset.\n",
    "\n",
    "Integer encoding of mask as follows:\n",
    "- Background => 0\n",
    "- Presynaptic process => 1\n",
    "- Synaptic cleft => 2\n",
    "- Postsynaptic process => 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from cloudvolume import CloudVolume, Bbox\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired image chunk size in px^3\n",
    "chunksize = 256\n",
    "\n",
    "# Define center pixel coordinates given chunk size\n",
    "xc_px = yc_px = zc_px = int(chunksize/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the full list of synapses and remove verified and unverified synapse IDs from above from this table\n",
    "all_syns = pd.read_csv('../../tables/pni_synapses_v185.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate this with a list of synapse IDs to download\n",
    "synid_list = all_syns.id.values[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mip level chosen for export (mip 1, res (x,y,z) = (8,8,40))\n",
    "mip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define v185 segmentation at chosen mip level\n",
    "# mip0 = (8,8,40) nm^3/vx\n",
    "segvol = CloudVolume('gs://microns_public_datasets/pinky100_v185/seg', mip=(mip-1), parallel=True, progress=True) # or mip-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synaptic cleft segmentation at chosen mip level\n",
    "# mip0 = (8,8,40) nm^3/vx\n",
    "cleftvol = CloudVolume('gs://neuroglancer/pinky100_v0/clefts/mip1_d2_1175k', mip=(mip-1), parallel=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_img_from_mask(nparr,targetfname):\n",
    "    # Takes a numpy array, with mask values 1 through 3\n",
    "    # and scales them up for easier viewing as embedded in a 256-bit\n",
    "    # image.\n",
    "    imgcurr = np.uint8(np.divide(nparr,np.max(nparr))*255)\n",
    "    img = Image.fromarray(imgcurr)\n",
    "    img.save(targetfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_mask_chunks(synid, cleftcv, segcv, chunksize, targetdir, is_img_test=0):\n",
    "    # Download an image chunk of size @chunksize centered on the synaptic cleft\n",
    "    # with segment ID @ synid\n",
    "    # Pull masks for the presynaptic cell (label 1) and postsynaptic cell (label 3)\n",
    "    # from the segmentation layer @segcv, and\n",
    "    # pull the mask for the synaptic cleft (label 2) from the synaptic cleft layer\n",
    "    # @cleftcv\n",
    "    # Save the summed mask as a .npy file in @targetdir    \n",
    "    # Currently, all_syns and mip are treated like globals.\n",
    "    \n",
    "    # Define label maps\n",
    "    presyn_label = 1\n",
    "    cleft_label = 2\n",
    "    postsyn_label = 3\n",
    "    \n",
    "    synrow = all_syns.loc[all_syns['id'] == synid]    \n",
    "    \n",
    "    # Get pre- and postsynaptic seg IDs for seg pulls\n",
    "    preid = list(synrow['pre_root_id'])[0]\n",
    "    postid = list(synrow['post_root_id'])[0]\n",
    "    \n",
    "    print(synid,preid,postid)\n",
    "    print(type(synid),type(preid),type(postid))\n",
    "    \n",
    "    # Get synapse centroid at full resolution ((x,y,z) = (4,4,40) nm3/vx)\n",
    "    x0 = list(synrow['ctr_pos_x_vx'])[0]\n",
    "    y0 = list(synrow['ctr_pos_y_vx'])[0]\n",
    "    z0 = list(synrow['ctr_pos_z_vx'])[0]\n",
    "\n",
    "    # Define bbox for export\n",
    "    xc = x0/(2**mip)\n",
    "    yc = y0/(2**mip)\n",
    "    \n",
    "    xtl = int(xc - chunksize/2)\n",
    "    xbr = int(xc + chunksize/2)\n",
    "    ytl = int(yc - chunksize/2)\n",
    "    ybr = int(yc + chunksize/2)\n",
    "    if is_img_test:\n",
    "        ztl = int(z0)\n",
    "        zbr = int(z0 + 1)\n",
    "        px_center = np.asarray([xc_px,yc_px])\n",
    "    else:\n",
    "        ztl = int(z0 - chunksize/2)\n",
    "        zbr = int(z0 + chunksize/2)\n",
    "        px_center = np.asarray([xc_px,yc_px,zc_px])\n",
    "   \n",
    "\n",
    "    # Generate synaptic cleft mask\n",
    "    cleftvol_dl = cleftcv.download(Bbox([xtl,ytl,ztl],[xbr,ybr,zbr]))\n",
    "    cleftvol = np.squeeze(np.asarray(cleftvol_dl))\n",
    "    # Map the synaptic cleft ID to the one used in this version of the synapse\n",
    "    # segmentation layer (same segments, different seg IDs)\n",
    "    # Do this by identifying the cleft closest to the synapse centroid\n",
    "    cleft_pxs = np.argwhere(cleftvol)\n",
    "    deltas = np.asarray([np.linalg.norm(q-px_center) for q in cleft_pxs])\n",
    "    closest_cleft = cleft_pxs[np.argmin(deltas)]\n",
    "    mapped_cleftid = cleftvol[tuple(closest_cleft)]\n",
    "    cleft_mask = np.uint8((cleftvol == mapped_cleftid))\n",
    "\n",
    "    # Generate pre- and postsynaptic cell masks\n",
    "    segvol_dl = segcv.download(Bbox([xtl,ytl,ztl],[xbr,ybr,zbr]))\n",
    "    segvol = np.squeeze(np.asarray(segvol_dl))\n",
    "    presyn_mask = np.uint8((segvol == preid))\n",
    "    postsyn_mask = np.uint8((segvol == postid))\n",
    "\n",
    "    # Combine masks (adding cleft labels last so they persist)\n",
    "    mask = postsyn_mask * postsyn_label\n",
    "    mask[presyn_mask.nonzero()] = presyn_label\n",
    "    mask[cleft_mask.nonzero()] = cleft_label\n",
    "    \n",
    "    if is_img_test:\n",
    "        # Print test masks\n",
    "        gen_test_img_from_mask(cleft_mask,os.path.join(targetdir,'testimg_cleft.png'))\n",
    "        gen_test_img_from_mask(presyn_mask,os.path.join(targetdir,'testimg_presyn.png'))\n",
    "        gen_test_img_from_mask(postsyn_mask,os.path.join(targetdir,'testimg_postsyn.png'))\n",
    "        gen_test_img_from_mask(mask,os.path.join(targetdir,'test_composite.png'))\n",
    "    else:\n",
    "        np.save(os.path.join(targetdir,'{0}_mask.npy'.format(synid)),mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and visualize test mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first synapse ID on list\n",
    "synid_test = synid_list[0]\n",
    "print(synid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targdir = '.'\n",
    "pull_mask_chunks(synid_test, cleftvol, segvol, chunksize, test_targdir, is_img_test=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up export for all synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targdir = 'path/for/exported/synapses'\n",
    "for synid in tqdm(synid_list):\n",
    "    pull_mask_chunks(synid,cleftvol,segvol,chunksize,targdir,is_img_test=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.load(os.path.join(targdir,'{0}_mask.npy'.format(synid_list[0])))\n",
    "print(np.shape(check))\n",
    "print(np.unique(check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up google cloud console information\n",
    "bucket_name = 'output/data/google/bucket/name'\n",
    "\n",
    "# Source for code below:\n",
    "# https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-code-sample\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
